{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a80b68-0afd-4e5b-8d63-a5535a494dc1",
   "metadata": {},
   "source": [
    "# Usando el autologger para no batallar\n",
    "\n",
    "## Pruebas sobre el uso de MLFlow\n",
    "\n",
    "### Aprendizaje Automático Aplicado\n",
    "\n",
    "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
    "\n",
    "**Julio Waissman**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917483a7-307a-452e-a2bc-8db896b28d63",
   "metadata": {},
   "source": [
    "## 1. Abriendo MLFlow y creando un experimento de chocolate\n",
    "\n",
    "Aqui en esta sección vamos a conctarnos al servidor de MLFlow y vamos a crear un experimento y una entrada de chocolate, solo para probar. Si ya existía el experimento (*chocolate* lo vamos a llamar) pues lo abre, y si no lo crea. \n",
    "\n",
    "Empecemos por cargar mlflow y generar el experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2bec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-2.3.2-py3-none-any.whl (17.7 MB)\n",
      "     --------------------------------------- 17.7/17.7 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz<2024 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (2022.1)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.2/41.2 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
      "     ------------------------------------- 224.5/224.5 kB 14.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Flask<3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.1.2)\n",
      "Requirement already satisfied: packaging<24 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (21.3)\n",
      "Collecting pyarrow<12,>=4.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "     --------------------------------------- 20.6/20.6 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.0.2)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\n",
      "     ---------------------------------------- 83.5/83.5 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting waitress<3\n",
      "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.7/57.7 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle<3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (8.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (2.28.1)\n",
      "Collecting docker<7,>=4.0.0\n",
      "  Downloading docker-6.1.2-py3-none-any.whl (148 kB)\n",
      "     -------------------------------------- 148.1/148.1 kB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.4.39)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "     ------------------------------------- 184.3/184.3 kB 11.6 MB/s eta 0:00:00\n",
      "Collecting protobuf<5,>=3.12.0\n",
      "  Downloading protobuf-4.23.1-cp39-cp39-win_amd64.whl (422 kB)\n",
      "     ------------------------------------- 422.5/422.5 kB 13.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (6.0)\n",
      "Collecting Jinja2<4,>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.1/133.1 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas<3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.4.4)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (3.3.4)\n",
      "Requirement already satisfied: numpy<2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.21.5)\n",
      "Requirement already satisfied: entrypoints<1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (3.5.2)\n",
      "Requirement already satisfied: scipy<2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.9.1)\n",
      "Collecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.3.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.7/78.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<9,>=7.0->mlflow) (0.4.5)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.4.0)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.11)\n",
      "Collecting pywin32>=304\n",
      "  Downloading pywin32-306-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "     ---------------------------------------- 9.3/9.3 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker<7,>=4.0.0->mlflow) (0.58.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask<3->mlflow) (2.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask<3->mlflow) (2.0.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (1.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (1.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143860 sha256=198071e8f6d2036ff6e1aed78ef0dafc1ecdd9b7fb83168f2b1d337b2cebc8bd\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\b6\\90\\68\\94d223a35a3910c1512a8d42d9f8333ce567ef26e250a56227\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: pywin32, waitress, sqlparse, smmap, querystring-parser, pyarrow, protobuf, oauthlib, Mako, Jinja2, gitdb, docker, databricks-cli, alembic, gitpython, mlflow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 12.0.0\n",
      "    Uninstalling pyarrow-12.0.0:\n",
      "      Successfully uninstalled pyarrow-12.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script waitress-serve.exe is installed in 'C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sqlformat.exe is installed in 'C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script plasma_store.exe is installed in 'C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\~yarrow\\\\arrow.dll'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6024668-e635-4155-848c-6b6e93cecb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Conectandose a MLFlow\n",
    "mlflow.set_tracking_uri(\"http://nodo3:45974\") \n",
    "\n",
    "\n",
    "# Generando el experimento o cargandolo si existe\n",
    "experiment_name = \"prueba\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Cargando la información\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Vamos a ver si es cierto\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"Tracking URI: {mlflow.tracking.get_tracking_uri()}\")\n",
    "print(f\"Nombre del experimento: {experiment_name}\")\n",
    "print(f\"ID del experimento: {experiment_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80375928-35c9-455d-966f-62efbc328df3",
   "metadata": {},
   "source": [
    "y ahora si vamos a generar datos para guardar la info de chocolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0eacd8-98f7-428f-b097-4a70e2c10c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vamos a usar las clases Param Metric y RunTag para ver opciones de registo\n",
    "from mlflow.entities import Param, Metric, RunTag\n",
    "\n",
    "def run(alpha, nombre_corrida, tiempo):\n",
    "    with mlflow.start_run(run_name=nombre_corrida) as run:\n",
    "        \n",
    "        # Guardando parámetros, métricas y tags de uno por uno\n",
    "        # Son inventados pues\n",
    "        mlflow.log_param(\"alpha\", str(alpha))\n",
    "        mlflow.log_metric(\"rmse\", 0.666)\n",
    "        mlflow.set_tag(\"nombre_corrida\", nombre_corrida)\n",
    "        \n",
    "        # Creando y guardando como artefato un archivo txt\n",
    "        with open(\"toto.txt\", \"w\") as f:\n",
    "            f.write(f\"Artifacto generado para {nombre_corrida}\")\n",
    "        mlflow.log_artifact(\"toto.txt\")\n",
    "        \n",
    "        # Como generar parametros, metricas y tags y guardarlas de un jalón\n",
    "        params = [ Param(\"p1\",\"0.1\"), Param(\"p2\",\"0.2\") ]\n",
    "        metrics = [ Metric(\"m1\", 0.1, tiempo, 0), Metric(\"m2\", 0.2, tiempo, 0) ]\n",
    "        tags = [ RunTag(\"t1\", \"hi1\"), RunTag(\"t2\", \"hi2\") ]\n",
    "        client.log_batch(run.info.run_uuid, metrics, params, tags)\n",
    "        \n",
    "        # Para ver que es lo que estamos guardando\n",
    "        print(f\"Id: {run.info.run_uuid}\")\n",
    "        print(f\"artifact_uri: {mlflow.get_artifact_uri()}\")\n",
    "        print(f\"alpha: {alpha}\")\n",
    "        print(f\"nombre de la corrida:\", nombre_corrida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97276dbc-2913-425c-a4e7-b9960e96d09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vamos a guardar la fecha\n",
    "import time\n",
    "now = round(time.time())\n",
    "\n",
    "run(3.1416, 'chocolata', now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11694a01-6dc9-40c0-a288-e95f97306e15",
   "metadata": {},
   "source": [
    "## 2. Usando autologger con sklearn\n",
    "\n",
    "Vamos a ver como funciona el autologger usando un `grid_seach` desde la librería `sklearn` para esto, vamos a utilizar el ejemplo de aprendizaje que [viene en la documentación de grid seach de scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py), pero registrandola en un experimento de MLFlow. Así podemos ver las ventajas de utilizar MLFlow para registrar los diferentes experimentos aunque sea en un problema de chocolate.\n",
    "\n",
    "Empecemos por crear el experimento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2609a5-5775-4b25-be7f-8c3f1a4f1ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generando el experimento o cargandolo si existe\n",
    "experiment_name = \"noticias\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Cargando la información\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Vamos a ver si es cierto\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"Tracking URI: {mlflow.tracking.get_tracking_uri()}\")\n",
    "print(f\"Nombre del experimento: {experiment_name}\")\n",
    "print(f\"ID del experimento: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59461b03-d7b1-4454-b9ea-06fd8831d0bc",
   "metadata": {},
   "source": [
    "Ahora vamos a descargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc20d9-710d-4871-9dcc-9d365d80eb31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "data = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "\n",
    "print(f\"{len(data.filenames)} documentos\")\n",
    "print(f\"{len(data.target_names)} categorías\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe49758-52e3-4d12-94d0-0987d9f46828",
   "metadata": {},
   "source": [
    "Por fin, el aprendizaje, tal como viene en la dicumentación, pero con mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab32df3-d7cd-4e13-955d-fe7ca80dfe15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def entrena(data, parametros=None):\n",
    "    mlflow.sklearn.autolog()\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"vect\", CountVectorizer()),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            (\"clf\", SGDClassifier()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if parametros == None:\n",
    "        parametros = {\n",
    "            \"vect__max_df\": (0.5, 0.75, 1.0),\n",
    "            \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "            \"clf__max_iter\": (20,),\n",
    "            \"clf__alpha\": (0.00001, 0.000001),\n",
    "            \"clf__penalty\": (\"l2\",),\n",
    "        }\n",
    "    \n",
    "    # Define y entrena con el autologger prendido\n",
    "    grid_search = GridSearchCV(pipeline, parametros, n_jobs=-1)\n",
    "    grid_search.fit(data.data, data.target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a0573-6685-426b-b56b-e4fb1af47790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parametros = {\n",
    "    \"vect__max_df\": (0.5, 0.75, 1.0),\n",
    "    \"vect__max_features\": (None, 5000, 10000, 50000),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    \"tfidf__use_idf\": (True, False),\n",
    "    #\"tfidf__norm\": ('l1', 'l2'),\n",
    "    \"clf__alpha\": (0.00001, 0.000001),\n",
    "    \"clf__penalty\": (\"l2\", \"elasticnet\"),\n",
    "    #\"clf__max_iter\": (20,),\n",
    "    \"clf__max_iter\": (20, 50, 80),\n",
    "}\n",
    "\n",
    "entrena(data, parametros)\n",
    "    \n",
    "# Imprimiendo donde se encuentra la información\n",
    "run_id = mlflow.last_active_run().info.run_id\n",
    "print(f\"Datos y modelo en la corrida: {run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8b569-def9-48f0-843d-d1c5396c0d9b",
   "metadata": {},
   "source": [
    "## 2. Autologer con Keras con un modelo desde cero\n",
    "\n",
    "Ahora vamos a hacer un modelito por default de los que usa uno cuando está aprendiando Keras y Tensorflow con el autologger, para ver como funciona de bonito. \n",
    "\n",
    "Empecemos bajando los datos de MINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01def8b-6a2b-44f8-8b42-f7eb99ed881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "def reshape(x, n):\n",
    "    x = x.reshape((n, 28 * 28))\n",
    "    return x.astype('float32') / 255\n",
    "\n",
    "x_train = reshape(x_train, x_train.shape[0])\n",
    "x_test = reshape(x_test, x_test.shape[0])\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "print(\"Data after reshape:\")\n",
    "print(\"  x_train.shape:\", x_train.shape)\n",
    "print(\"  y_train.shape:\", y_train.shape)\n",
    "print(\"  x_test.shape:\", x_test.shape)\n",
    "print(\"  y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69805ab0-a485-4e42-9888-a6037a894198",
   "metadata": {},
   "source": [
    "y ahora creemos o abramos el experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f2171-3393-4568-a906-946441440f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generando el experimento o cargandolo si existe\n",
    "experiment_name = \"minst-keras\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Cargando la información\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Vamos a ver si es cierto\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"Tracking URI: {mlflow.tracking.get_tracking_uri()}\")\n",
    "print(f\"Nombre del experimento: {experiment_name}\")\n",
    "print(f\"ID del experimento: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b5037-39be-4d90-8fd5-fd6155ac7063",
   "metadata": {},
   "source": [
    "y ahora las funciones para el aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aadc6a-9fef-4ba2-bae9-8363f8e96488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "\n",
    "def entrena_keras_MINST(x_train, y_train, x_test, y_test, \n",
    "                        epochs, batch_size, seed=42, autolog='keras'):\n",
    "\n",
    "    # Para reproducibilkidad, si seed es None, no se pone\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "    \n",
    "    if autolog == 'keras':\n",
    "        mlflow.keras.autolog()\n",
    "    elif autolog == 'tensorflow':\n",
    "        mlflow.tensorflow.autolog()\n",
    "    else:\n",
    "        mlflow.autolog()\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.set_tag(\"version.mlflow\", mlflow.__version__)\n",
    "        mlflow.set_tag(\"version.keras\", keras.__version__)\n",
    "        mlflow.set_tag(\"version.tensorflow\", tf.__version__)\n",
    "        mlflow.set_tag(\"autolog\", autolog)\n",
    "        mlflow.set_tag(\"seed\", f\"{seed}\")\n",
    "        \n",
    "        # Un modelito bien simple solo por probar\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(512, activation=\"relu\", input_shape=(28 * 28,)))\n",
    "        model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "        \n",
    "        # Compilando y entrenando\n",
    "        model.compile(\n",
    "            optimizer=\"rmsprop\",\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "        \n",
    "        # Guardando el modelo en formato json\n",
    "        with open(\"model.json\", \"w\") as f:\n",
    "            f.write(model.to_json())\n",
    "        mlflow.log_artifact(\"model.json\")\n",
    "        mlflow.log_metrics(\n",
    "            {'test_loss': test_loss, 'test_acc':test_acc}\n",
    "        )\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774716e9-aeee-4f5b-925b-352efcc98853",
   "metadata": {},
   "source": [
    "y ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdb1b0-bae0-46d4-b28d-e3a8e046437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "entrena_keras_MINST(\n",
    "    x_train, y_train, x_test, y_test, \n",
    "    epochs, batch_size, seed=42,\n",
    "    autolog='tensorflow'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25339a-8722-4b97-a4c3-d5a75a4be631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
